{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from common.common import create_folder,H5Recorder\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "\n",
    "from model import optimiser\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from common.common import load_obj\n",
    "from model.utils import age_vocab\n",
    "from dataLoader.NextXVisit import NextVisit\n",
    "from model.NextXVisit import BertForSingleLabelPrediction\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the parquet file\n",
    "file_path = \"/data/datasets/leyang.sun/merged_age_diagnosis.csv\"\n",
    "\n",
    "\n",
    "# Read the DataFrame from the parquet file\n",
    "original_data = pd.read_csv(file_path)\n",
    "\n",
    "original_data['age_vector'] = original_data['age_vector'].apply(lambda x: ''.join([char for char in str(x) if (char != ' ' and char != '[' and char != ']')]).split(','))\n",
    "original_data['age_vector'] = original_data['age_vector'].apply(lambda x: list(map(int, x)))\n",
    "\n",
    "original_data['diagnosis_code'] = original_data['diagnosis_code'].apply(lambda x: ''.join([char for char in str(x) if (char != ' ' and char != '[' and char != ']')]).split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0             deid_pat_ID  \\\n",
      "0              0      IRB202001139_PAT_1   \n",
      "1              1     IRB202001139_PAT_10   \n",
      "2              2  IRB202001139_PAT_10001   \n",
      "3              3  IRB202001139_PAT_10002   \n",
      "4              4  IRB202001139_PAT_10009   \n",
      "...          ...                     ...   \n",
      "9916        9916   IRB202001139_PAT_9958   \n",
      "9917        9917   IRB202001139_PAT_9965   \n",
      "9918        9918   IRB202001139_PAT_9977   \n",
      "9919        9919   IRB202001139_PAT_9992   \n",
      "9920        9920   IRB202001139_PAT_9993   \n",
      "\n",
      "                                             age_vector  \\\n",
      "0     [31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 3...   \n",
      "1     [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3     [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4     [66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 6...   \n",
      "...                                                 ...   \n",
      "9916                               [62, 62, 62, 62, 62]   \n",
      "9917  [77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 7...   \n",
      "9918  [9, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...   \n",
      "9919                                               [59]   \n",
      "9920  [86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 8...   \n",
      "\n",
      "                                         diagnosis_code  \n",
      "0     ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...  \n",
      "1     ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...  \n",
      "2     ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...  \n",
      "3     ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...  \n",
      "4     ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...  \n",
      "...                                                 ...  \n",
      "9916  ['CLS', 159.0, 162.0, 164.0, 315.0, 479.0, 492...  \n",
      "9917  ['CLS', 119.0, 159.0, 162.0, 164.0, 172.0, 'SE...  \n",
      "9918  ['CLS', 1.0, 1.0, 366.0, 392.0, 392.0, 498.0, ...  \n",
      "9919  ['CLS', 10.0, 71.0, 315.0, 352.0, 352.0, 366.0...  \n",
      "9920  ['CLS', 345.0, 760.0, 775.0, 813.0, 813.0, 830...  \n",
      "\n",
      "[9921 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(original_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "directory_path = '/data/datasets/leyang.sun/BEHRT_validation'\n",
    "\n",
    "# Read CSV files and create a dictionary to store 'deid_pat_ID' from each file\n",
    "pid_dict = {}\n",
    "for i in range(1, 7):\n",
    "    file_path = os.path.join(directory_path, f'file_{i}.csv')\n",
    "    df = pd.read_csv(file_path)\n",
    "    pid_dict[f'file_{i}'] = set(df['deid_pat_ID'])\n",
    "\n",
    "\n",
    "# Assuming original_data is your DataFrame\n",
    "original_data['label'] = original_data['deid_pat_ID'].apply(lambda x: 1 if x in pid_dict[f'file_{1}'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0             deid_pat_ID  \\\n",
      "0              0      IRB202001139_PAT_1   \n",
      "1              1     IRB202001139_PAT_10   \n",
      "2              2  IRB202001139_PAT_10001   \n",
      "3              3  IRB202001139_PAT_10002   \n",
      "4              4  IRB202001139_PAT_10009   \n",
      "...          ...                     ...   \n",
      "9916        9916   IRB202001139_PAT_9958   \n",
      "9917        9917   IRB202001139_PAT_9965   \n",
      "9918        9918   IRB202001139_PAT_9977   \n",
      "9919        9919   IRB202001139_PAT_9992   \n",
      "9920        9920   IRB202001139_PAT_9993   \n",
      "\n",
      "                                             age_vector  \\\n",
      "0     [31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 3...   \n",
      "1     [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3     [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4     [66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 6...   \n",
      "...                                                 ...   \n",
      "9916                               [62, 62, 62, 62, 62]   \n",
      "9917  [77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 7...   \n",
      "9918  [9, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...   \n",
      "9919                                               [59]   \n",
      "9920  [86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 8...   \n",
      "\n",
      "                                         diagnosis_code  label  \n",
      "0     ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...      1  \n",
      "1     ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...      1  \n",
      "2     ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...      0  \n",
      "3     ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...      1  \n",
      "4     ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...      1  \n",
      "...                                                 ...    ...  \n",
      "9916  ['CLS', 159.0, 162.0, 164.0, 315.0, 479.0, 492...      0  \n",
      "9917  ['CLS', 119.0, 159.0, 162.0, 164.0, 172.0, 'SE...      0  \n",
      "9918  ['CLS', 1.0, 1.0, 366.0, 392.0, 392.0, 498.0, ...      0  \n",
      "9919  ['CLS', 10.0, 71.0, 315.0, 352.0, 352.0, 366.0...      0  \n",
      "9920  ['CLS', 345.0, 760.0, 775.0, 813.0, 813.0, 830...      0  \n",
      "\n",
      "[9921 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(original_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test datasets\n",
    "\n",
    "file_config = {\n",
    "    'vocab': '/home/leyang.sun/BERHT/BEHRT/saved_vocab', # token2idx idx2token\n",
    "    'train': '/home/leyang.sun/BERHT/BEHRT/train_data.parquet',\n",
    "    'test': '/home/leyang.sun/BERHT/BEHRT/test_data.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(row):\n",
    "    # Count the number of visits for the patient\n",
    "    total_visits = row['diagnosis_code'].count(\"'SEP'\") \n",
    "\n",
    "    # Check if total visits is greater than 3\n",
    "    if total_visits <= 3:\n",
    "        return None\n",
    "\n",
    "    x_p = row['age_vector']\n",
    "\n",
    "    label = row['label']\n",
    "\n",
    "    # Delete elements after the jth 'SEP'\n",
    "    row['diagnosis_code'] = row['diagnosis_code']\n",
    "\n",
    "    return pd.Series({'deid_pat_ID': row['deid_pat_ID'], 'age_vector': x_p, 'diagnosis_code': row['diagnosis_code'], 'label': label})\n",
    "\n",
    "# Apply the function to each row of the original data\n",
    "processed_data = original_data.apply(process_patient_data, axis=1)\n",
    "\n",
    "# Drop rows where total visits is less than or equal to 3\n",
    "processed_data = processed_data.dropna()\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "processed_data_df = pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               deid_pat_ID                                         age_vector  \\\n",
      "0       IRB202001139_PAT_1  [31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 3...   \n",
      "1      IRB202001139_PAT_10  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2   IRB202001139_PAT_10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3   IRB202001139_PAT_10002  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4   IRB202001139_PAT_10009  [66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 6...   \n",
      "6   IRB202001139_PAT_10014                                   [64, 65, 68, 68]   \n",
      "7   IRB202001139_PAT_10018  [58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 5...   \n",
      "8   IRB202001139_PAT_10020                   [83, 83, 83, 83, 83, 84, 84, 84]   \n",
      "9   IRB202001139_PAT_10025  [48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...   \n",
      "10  IRB202001139_PAT_10034  [48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 5...   \n",
      "11  IRB202001139_PAT_10037                                   [46, 46, 46, 46]   \n",
      "12  IRB202001139_PAT_10039  [21, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...   \n",
      "13   IRB202001139_PAT_1004  [30, 32, 32, 32, 32, 32, 32, 32, 33, 34, 34, 3...   \n",
      "14  IRB202001139_PAT_10040  [77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 7...   \n",
      "15  IRB202001139_PAT_10041  [59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 6...   \n",
      "16  IRB202001139_PAT_10046  [62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 6...   \n",
      "17  IRB202001139_PAT_10047                                   [45, 45, 45, 45]   \n",
      "18  IRB202001139_PAT_10048  [81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 8...   \n",
      "19  IRB202001139_PAT_10054       [69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]   \n",
      "21  IRB202001139_PAT_10056  [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 5...   \n",
      "\n",
      "                                       diagnosis_code  label  \n",
      "0   ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...    1.0  \n",
      "1   ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...    1.0  \n",
      "2   ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...    0.0  \n",
      "3   ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...    1.0  \n",
      "4   ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...    1.0  \n",
      "6   ['CLS', 1532.0, 1696.0, 1696.0, 'SEP', 760.0, ...    1.0  \n",
      "7   ['CLS', 570.0, 'SEP', 570.0, 'SEP', 760.0, 150...    1.0  \n",
      "8   ['CLS', 236.0, 760.0, 1564.0, 1696.0, 'SEP', 4...    0.0  \n",
      "9   ['CLS', 345.0, 499.0, 760.0, 775.0, 775.0, 814...    1.0  \n",
      "10  ['CLS', 345.0, 398.0, 761.0, 1696.0, 'SEP', 66...    1.0  \n",
      "11  ['CLS', 540.0, 760.0, 840.0, 844.0, 865.0, 865...    1.0  \n",
      "12  ['CLS', 1720.0, 1720.0, 'SEP', 71.0, 1038.0, 1...    1.0  \n",
      "13  ['CLS', 936.0, 1271.0, 1320.0, 1696.0, 71.0, 1...    0.0  \n",
      "14  ['CLS', 653.0, 760.0, 1145.0, 1158.0, 1204.0, ...    1.0  \n",
      "15  ['CLS', 1902.0, 1902.0, 1920.0, 1920.0, 345.0,...    1.0  \n",
      "16  ['CLS', 126.0, 126.0, 'SEP', 126.0, 741.0, 'SE...    1.0  \n",
      "17  ['CLS', 176.0, 250.0, 570.0, 829.0, 1753.0, 17...    0.0  \n",
      "18  ['CLS', 557.0, 'SEP', 557.0, 'SEP', 557.0, 'SE...    0.0  \n",
      "19  ['CLS', 328.0, 366.0, 366.0, 415.0, 492.0, 502...    1.0  \n",
      "21  ['CLS', 10.0, 760.0, 791.0, 1867.0, 1862.0, 35...    0.0  \n"
     ]
    }
   ],
   "source": [
    "print(processed_data_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   deid_pat_ID                                         age_vector  \\\n",
      "0            1  [31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 3...   \n",
      "1           10  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2        10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3        10002  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4        10009  [66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 6...   \n",
      "6        10014                                   [64, 65, 68, 68]   \n",
      "7        10018  [58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 5...   \n",
      "8        10020                   [83, 83, 83, 83, 83, 84, 84, 84]   \n",
      "9        10025  [48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...   \n",
      "10       10034  [48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 5...   \n",
      "11       10037                                   [46, 46, 46, 46]   \n",
      "12       10039  [21, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...   \n",
      "13        1004  [30, 32, 32, 32, 32, 32, 32, 32, 33, 34, 34, 3...   \n",
      "14       10040  [77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 7...   \n",
      "15       10041  [59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 6...   \n",
      "16       10046  [62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 6...   \n",
      "17       10047                                   [45, 45, 45, 45]   \n",
      "18       10048  [81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 8...   \n",
      "19       10054       [69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]   \n",
      "21       10056  [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 5...   \n",
      "\n",
      "                                       diagnosis_code label  \n",
      "0   ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...   1.0  \n",
      "1   ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...   1.0  \n",
      "2   ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...   0.0  \n",
      "3   ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...   1.0  \n",
      "4   ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...   1.0  \n",
      "6   ['CLS', 1532.0, 1696.0, 1696.0, 'SEP', 760.0, ...   1.0  \n",
      "7   ['CLS', 570.0, 'SEP', 570.0, 'SEP', 760.0, 150...   1.0  \n",
      "8   ['CLS', 236.0, 760.0, 1564.0, 1696.0, 'SEP', 4...   0.0  \n",
      "9   ['CLS', 345.0, 499.0, 760.0, 775.0, 775.0, 814...   1.0  \n",
      "10  ['CLS', 345.0, 398.0, 761.0, 1696.0, 'SEP', 66...   1.0  \n",
      "11  ['CLS', 540.0, 760.0, 840.0, 844.0, 865.0, 865...   1.0  \n",
      "12  ['CLS', 1720.0, 1720.0, 'SEP', 71.0, 1038.0, 1...   1.0  \n",
      "13  ['CLS', 936.0, 1271.0, 1320.0, 1696.0, 71.0, 1...   0.0  \n",
      "14  ['CLS', 653.0, 760.0, 1145.0, 1158.0, 1204.0, ...   1.0  \n",
      "15  ['CLS', 1902.0, 1902.0, 1920.0, 1920.0, 345.0,...   1.0  \n",
      "16  ['CLS', 126.0, 126.0, 'SEP', 126.0, 741.0, 'SE...   1.0  \n",
      "17  ['CLS', 176.0, 250.0, 570.0, 829.0, 1753.0, 17...   0.0  \n",
      "18  ['CLS', 557.0, 'SEP', 557.0, 'SEP', 557.0, 'SE...   0.0  \n",
      "19  ['CLS', 328.0, 366.0, 366.0, 415.0, 492.0, 502...   1.0  \n",
      "21  ['CLS', 10.0, 760.0, 791.0, 1867.0, 1862.0, 35...   0.0  \n"
     ]
    }
   ],
   "source": [
    "processed_data_df['deid_pat_ID'] = processed_data_df['deid_pat_ID'].str.replace('IRB202001139_PAT_', '', regex=False)\n",
    "processed_data_df['label'] = processed_data_df['label'].astype(str)\n",
    "print(processed_data_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(processed_data_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train_df = train_df.rename(columns={'age_vector': 'age', 'diagnosis_code': 'code', 'deid_pat_ID':'patid'})\n",
    "test_df = test_df.rename(columns={'age_vector': 'age', 'diagnosis_code': 'code', 'deid_pat_ID':'patid'})\n",
    "\n",
    "\n",
    "# Reset the index of train and test DataFrames\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save DataFrames as Parquet files without the index column\n",
    "train_df.to_parquet('/home/leyang.sun/BERHT/BEHRT/train_data.parquet', index=False)\n",
    "test_df.to_parquet('/home/leyang.sun/BERHT/BEHRT/test_data.parquet', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      patid                                                age  \\\n",
      "0     31613                                   [84, 85, 85, 85]   \n",
      "1     17615                                   [64, 64, 65, 65]   \n",
      "2     42668  [65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 6...   \n",
      "3     17281  [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 5...   \n",
      "4     13322  [53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 5...   \n",
      "...     ...                                                ...   \n",
      "6353  38660  [60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 6...   \n",
      "6354  39587  [66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 6...   \n",
      "6355  14715  [83, 83, 84, 84, 85, 85, 87, 87, 87, 87, 87, 8...   \n",
      "6356   8124                               [82, 84, 84, 84, 88]   \n",
      "6357   6316       [38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39]   \n",
      "\n",
      "                                                   code label  \n",
      "0     ['CLS', 366.0, 366.0, 760.0, 1231.0, 1696.0, 1...   0.0  \n",
      "1     ['CLS', 1523.0, 'SEP', 1550.0, 1700.0, 'SEP', ...   0.0  \n",
      "2     ['CLS', 35.0, 35.0, 1917.0, 1920.0, 1885.0, 19...   1.0  \n",
      "3     ['CLS', 195.0, 492.0, 760.0, 1053.0, 1051.0, 1...   0.0  \n",
      "4     ['CLS', 1421.0, 73.0, 1975.0, 1677.0, 2016.0, ...   0.0  \n",
      "...                                                 ...   ...  \n",
      "6353  ['CLS', 73.0, 1947.0, 'SEP', 1882.0, 1902.0, '...   0.0  \n",
      "6354  ['CLS', 1915.0, 1915.0, 1915.0, 1915.0, 366.0,...   1.0  \n",
      "6355  ['CLS', 1555.0, 'SEP', 1796.0, 'SEP', 612.0, 6...   0.0  \n",
      "6356  ['CLS', 1796.0, 1801.0, 'SEP', 10.0, 16.0, 458...   1.0  \n",
      "6357  ['CLS', 1.0, 446.0, 481.0, 485.0, 553.0, 557.0...   1.0  \n",
      "\n",
      "[6358 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      patid                                                age  \\\n",
      "0     16496                                   [88, 88, 88, 88]   \n",
      "1     22754               [53, 53, 53, 53, 53, 53, 55, 55, 55]   \n",
      "2     28424  [68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...   \n",
      "3     17950  [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 6...   \n",
      "4     28404  [72, 72, 72, 73, 73, 73, 74, 74, 74, 74, 74, 7...   \n",
      "...     ...                                                ...   \n",
      "1585  11803  [65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 6...   \n",
      "1586  10574  [73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 7...   \n",
      "1587  38265  [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 3...   \n",
      "1588  47558                                   [84, 84, 84, 85]   \n",
      "1589  16019  [74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 7...   \n",
      "\n",
      "                                                   code label  \n",
      "0     ['CLS', 345.0, 576.0, 775.0, 891.0, 902.0, 123...   0.0  \n",
      "1     ['CLS', 760.0, 775.0, 'SEP', 760.0, 775.0, 'SE...   1.0  \n",
      "2     ['CLS', 775.0, 1696.0, 'SEP', 775.0, 775.0, 'S...   1.0  \n",
      "3     ['CLS', 22.0, 1231.0, 1555.0, 1709.0, 1.0, 10....   0.0  \n",
      "4     ['CLS', 1920.0, 1087.0, 1677.0, 1705.0, 1707.0...   1.0  \n",
      "...                                                 ...   ...  \n",
      "1585  ['CLS', 345.0, 558.0, 1113.0, 1697.0, 1706.0, ...   0.0  \n",
      "1586  ['CLS', 236.0, 250.0, 315.0, 321.0, 322.0, 345...   0.0  \n",
      "1587  ['CLS', 1920.0, 1922.0, 1923.0, 1903.0, 1957.0...   1.0  \n",
      "1588  ['CLS', 1916.0, 345.0, 1928.0, 1928.0, 1929.0,...   0.0  \n",
      "1589  ['CLS', 761.0, 1705.0, 'SEP', 761.0, 937.0, 12...   0.0  \n",
      "\n",
      "[1590 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 5e-4,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': '/home/leyang.sun/BERHT/BEHRT/fine_tuned_model',# output folder\n",
    "    'best_name': 'FineTuned_BERT_Large_Nextvisit',  # output model name\n",
    "    'max_len_seq': 100,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "pretrain_model_path = '/home/leyang.sun/BERHT/BEHRT/saved_model/BERT_Large_v1_2023-10-19'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.0': 0, '1.0': 1, 'UNK': 2}\n"
     ]
    }
   ],
   "source": [
    "labelVocab = {}\n",
    "label_token = ['0.0','1.0', 'UNK']\n",
    "for i,x in enumerate(label_token):\n",
    "    labelVocab[x] = i\n",
    "print(labelVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "    'num_labels': 1\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word':True,\n",
    "    'seg':True,\n",
    "    'age':True,\n",
    "    'position': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')  # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(file_config['train'])\n",
    "Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=train, max_len=global_params['max_len_seq'])\n",
    "trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(file_config['test'])\n",
    "Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=test, max_len=global_params['max_len_seq'])\n",
    "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForSingleLabelPrediction(conf, num_labels=len(labelVocab.keys()), feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(path, model):\n",
    "#     # load pretrained model and update weights\n",
    "#     pretrained_dict = torch.load(path)\n",
    "#     model_dict = model.state_dict()\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict)\n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "#     return model\n",
    "\n",
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out unnecessary keys and skip the mismatched parameter\n",
    "    pretrained_dict = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in model_dict and k != 'bert.embeddings.posi_embeddings.weight'\n",
    "    }\n",
    "\n",
    "    # Update entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "\n",
    "    # Load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "mode = load_model(pretrain_model_path, model)  # Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = model.to(global_params['device'])\n",
    "optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import sklearn\n",
    "# def precision(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "#     label, output=label.cpu(), output.detach().cpu()\n",
    "#     tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "#     return tempprc, output, label\n",
    "\n",
    "# def precision_test(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "#     tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "#     roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "#     return tempprc, roc, output, label\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits).detach().cpu().numpy()\n",
    "    label = label.cpu().numpy()\n",
    "    accuracy = accuracy_score(label, (output > 0.5).astype(int))\n",
    "    return accuracy\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits).detach().cpu().numpy()\n",
    "    label = label.cpu().numpy()\n",
    "\n",
    "    if len(np.unique(label)) == 1:\n",
    "        # Handle the case when there is only one class\n",
    "        return 0.0, 0.0, output, label\n",
    "\n",
    "    tempprc = sklearn.metrics.average_precision_score(label, output, average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label, output, average='samples')\n",
    "\n",
    "    return tempprc, roc, output, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "# mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(e):\n",
    "#     model.train()\n",
    "#     tr_loss = 0\n",
    "#     temp_loss = 0\n",
    "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#     cnt = 0\n",
    "#     for step, batch in enumerate(trainload):\n",
    "#         cnt +=1\n",
    "#         age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        \n",
    "#         targets = targets.to(torch.long).to(global_params['device'])\n",
    "#         age_ids = age_ids.to(global_params['device'])\n",
    "#         input_ids = input_ids.to(global_params['device'])\n",
    "#         posi_ids = posi_ids.to(global_params['device'])\n",
    "#         segment_ids = segment_ids.to(global_params['device'])\n",
    "#         attMask = attMask.to(global_params['device'])\n",
    "#         targets = targets.to(global_params['device'], dtype=torch.long)\n",
    "        \n",
    "#         loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        \n",
    "#         if global_params['gradient_accumulation_steps'] >1:\n",
    "#             loss = loss/global_params['gradient_accumulation_steps']\n",
    "#         loss.backward()\n",
    "        \n",
    "#         temp_loss += loss.item()\n",
    "#         tr_loss += loss.item()\n",
    "#         nb_tr_examples += input_ids.size(0)\n",
    "#         nb_tr_steps += 1\n",
    "        \n",
    "#         if step % 500==0:\n",
    "#             prec, a, b = precision(logits, targets)\n",
    "#             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt,temp_loss/500, prec))\n",
    "#             temp_loss = 0\n",
    "        \n",
    "#         if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "#             optim.step()\n",
    "#             optim.zero_grad()\n",
    "\n",
    "# def train(e):\n",
    "#     model.train()\n",
    "#     tr_loss = 0\n",
    "#     temp_loss = 0\n",
    "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#     cnt = 0\n",
    "#     for step, batch in enumerate(trainload):\n",
    "#         cnt += 1\n",
    "#         age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        \n",
    "#         targets = targets.to(torch.long).to(global_params['device'])\n",
    "#         age_ids = age_ids.to(global_params['device'])\n",
    "#         input_ids = input_ids.to(global_params['device'])\n",
    "#         posi_ids = posi_ids.to(global_params['device'])\n",
    "#         segment_ids = segment_ids.to(global_params['device'])\n",
    "#         attMask = attMask.to(global_params['device'])\n",
    "#         targets = targets.to(global_params['device'], dtype=torch.long)\n",
    "        \n",
    "#         loss, logits = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        \n",
    "#         if global_params['gradient_accumulation_steps'] > 1:\n",
    "#             loss = loss / global_params['gradient_accumulation_steps']\n",
    "#         loss.backward()\n",
    "        \n",
    "#         temp_loss += loss.item()\n",
    "#         tr_loss += loss.item()\n",
    "#         nb_tr_examples += input_ids.size(0)\n",
    "#         nb_tr_steps += 1\n",
    "        \n",
    "#         if step % 500 == 0:\n",
    "#             acc = precision(logits, targets)\n",
    "#             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| Accuracy: {}\".format(e, cnt, temp_loss / 500, acc))\n",
    "#             temp_loss = 0\n",
    "        \n",
    "#         if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "#             optim.step()\n",
    "#             optim.zero_grad()\n",
    "            \n",
    "            \n",
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        \n",
    "        targets = targets.to(torch.long).to(global_params['device'])\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'], dtype=torch.long)\n",
    "        \n",
    "        loss, logits = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            acc = precision(logits, targets)\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| Accuracy: {}\".format(e, cnt, temp_loss / 500, acc))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            \n",
    "            \n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(testload):\n",
    "        model.eval()\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'], dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, output, label, tr_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Cnt: 1\t| Loss: 0.013340864181518555\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 1\t| Cnt: 1\t| Loss: 0.013192417144775391\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 2\t| Cnt: 1\t| Loss: 0.013188467979431153\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 3\t| Cnt: 1\t| Loss: 0.013186370849609375\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 4\t| Cnt: 1\t| Loss: 0.013184869766235351\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 5\t| Cnt: 1\t| Loss: 0.013184259414672852\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 6\t| Cnt: 1\t| Loss: 0.013183910369873047\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 7\t| Cnt: 1\t| Loss: 0.013183755874633789\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 8\t| Cnt: 1\t| Loss: 0.013183581352233886\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 9\t| Cnt: 1\t| Loss: 0.013183537483215333\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 10\t| Cnt: 1\t| Loss: 0.013183506965637207\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 11\t| Cnt: 1\t| Loss: 0.013183453559875487\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 12\t| Cnt: 1\t| Loss: 0.013183441162109375\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 13\t| Cnt: 1\t| Loss: 0.013183416366577149\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 14\t| Cnt: 1\t| Loss: 0.013183405876159668\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 15\t| Cnt: 1\t| Loss: 0.01318339729309082\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 16\t| Cnt: 1\t| Loss: 0.013183391571044922\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 17\t| Cnt: 1\t| Loss: 0.013183389663696289\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 18\t| Cnt: 1\t| Loss: 0.013183381080627441\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 19\t| Cnt: 1\t| Loss: 0.013183380126953125\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 20\t| Cnt: 1\t| Loss: 0.013183381080627441\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 21\t| Cnt: 1\t| Loss: 0.013183378219604492\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 22\t| Cnt: 1\t| Loss: 0.01318337345123291\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 23\t| Cnt: 1\t| Loss: 0.013183378219604492\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 24\t| Cnt: 1\t| Loss: 0.013183378219604492\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 25\t| Cnt: 1\t| Loss: 0.013183374404907227\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 26\t| Cnt: 1\t| Loss: 0.013183371543884277\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 27\t| Cnt: 1\t| Loss: 0.01318337631225586\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 28\t| Cnt: 1\t| Loss: 0.013183372497558594\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 29\t| Cnt: 1\t| Loss: 0.01318337059020996\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 30\t| Cnt: 1\t| Loss: 0.013183368682861327\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 31\t| Cnt: 1\t| Loss: 0.013183369636535644\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 32\t| Cnt: 1\t| Loss: 0.013183367729187013\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 33\t| Cnt: 1\t| Loss: 0.01318337059020996\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 34\t| Cnt: 1\t| Loss: 0.013183366775512696\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 35\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 36\t| Cnt: 1\t| Loss: 0.013183366775512696\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 37\t| Cnt: 1\t| Loss: 0.013183364868164063\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 38\t| Cnt: 1\t| Loss: 0.01318336582183838\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 39\t| Cnt: 1\t| Loss: 0.013183367729187013\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 40\t| Cnt: 1\t| Loss: 0.013183363914489746\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 41\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 42\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 43\t| Cnt: 1\t| Loss: 0.013183363914489746\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 44\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 45\t| Cnt: 1\t| Loss: 0.013183361053466796\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 46\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 47\t| Cnt: 1\t| Loss: 0.01318336296081543\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 48\t| Cnt: 1\t| Loss: 0.013183361053466796\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 49\t| Cnt: 1\t| Loss: 0.013183361053466796\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 50\t| Cnt: 1\t| Loss: 0.013183362007141113\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 51\t| Cnt: 1\t| Loss: 0.01318336009979248\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 52\t| Cnt: 1\t| Loss: 0.013183359146118165\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 53\t| Cnt: 1\t| Loss: 0.013183358192443848\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 54\t| Cnt: 1\t| Loss: 0.01318336009979248\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 55\t| Cnt: 1\t| Loss: 0.013183358192443848\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 56\t| Cnt: 1\t| Loss: 0.013183361053466796\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 57\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 58\t| Cnt: 1\t| Loss: 0.01318336009979248\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 59\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 60\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 61\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 62\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 63\t| Cnt: 1\t| Loss: 0.013183358192443848\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 64\t| Cnt: 1\t| Loss: 0.013183361053466796\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 65\t| Cnt: 1\t| Loss: 0.013183356285095215\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 66\t| Cnt: 1\t| Loss: 0.013183357238769532\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 67\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 68\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 69\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 70\t| Cnt: 1\t| Loss: 0.013183354377746582\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 71\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 72\t| Cnt: 1\t| Loss: 0.013183356285095215\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 73\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 74\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 75\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 76\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 77\t| Cnt: 1\t| Loss: 0.013183354377746582\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 78\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 79\t| Cnt: 1\t| Loss: 0.013183355331420898\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 80\t| Cnt: 1\t| Loss: 0.013183354377746582\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 81\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 82\t| Cnt: 1\t| Loss: 0.013183352470397949\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 83\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 84\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 85\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 91\t| Cnt: 1\t| Loss: 0.013183352470397949\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 92\t| Cnt: 1\t| Loss: 0.013183350563049317\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 93\t| Cnt: 1\t| Loss: 0.013183351516723632\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 94\t| Cnt: 1\t| Loss: 0.013183351516723632\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 95\t| Cnt: 1\t| Loss: 0.013183351516723632\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 96\t| Cnt: 1\t| Loss: 0.013183351516723632\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 97\t| Cnt: 1\t| Loss: 0.013183353424072265\t| Accuracy: 0.0\n",
      "aps : 0.0\n",
      "epoch: 98\t| Cnt: 1\t| Loss: 0.013183351516723632\t| Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# best_pre = 0.0\n",
    "# for e in range(200):\n",
    "#     train(e)\n",
    "#     acc, aps, roc, test_loss = evaluation()\n",
    "#     if aps >best_pre:\n",
    "        \n",
    "#         # Save a trained model\n",
    "#         print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "#         model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "#         output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "#         create_folder(global_params['output_dir'])\n",
    "\n",
    "#         torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#         best_pre = aps\n",
    "#     print('aps : {}'.format(aps))\n",
    " \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "best_aps = 0.0  # Change variable name from best_pre to best_aps\n",
    "for e in range(300):\n",
    "    train(e)\n",
    "    aps, roc, output, label, test_loss = evaluation()  # Update the function call\n",
    "    if aps > best_aps:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        output_model_file = os.path.join(global_params['output_dir'], global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_aps = aps\n",
    "    print('aps : {}'.format(aps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-leyang.sun",
   "language": "python",
   "name": "leyang.sun-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
