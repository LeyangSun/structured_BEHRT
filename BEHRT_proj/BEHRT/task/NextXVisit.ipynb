{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from common.common import create_folder,H5Recorder\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "\n",
    "from model import optimiser\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from common.common import load_obj\n",
    "from model.utils import age_vocab\n",
    "from dataLoader.NextXVisit import NextVisit\n",
    "from model.NextXVisit import BertForMultiLabelPrediction\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the parquet file\n",
    "file_path = \"/data/datasets/leyang.sun/merged_age_diagnosis.csv\"\n",
    "\n",
    "\n",
    "# Read the DataFrame from the parquet file\n",
    "original_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data['age_vector'] = original_data['age_vector'].apply(lambda x: ''.join([char for char in str(x) if (char != ' ' and char != '[' and char != ']')]).split(','))\n",
    "original_data['age_vector'] = original_data['age_vector'].apply(lambda x: list(map(int, x)))\n",
    "\n",
    "# Display the result for the first row\n",
    "\n",
    "#print(original_data['age_vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 33, 33, 33, 35, 35]\n"
     ]
    }
   ],
   "source": [
    "print(original_data['age_vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'CLS'\", '366.0', '366.0', '418.0', '705.0', '760.0', '838.0', '840.0', '844.0', '994.0', '1181.0', '1564.0', '1705.0', '1719.0', \"'SEP'\", '366.0', '760.0', '1181.0', '1564.0', \"'SEP'\", '579.0', \"'SEP'\", '579.0', \"'SEP'\", '838.0', '1729.0', \"'SEP'\", '840.0', \"'SEP'\", '1943.0', '1941.0', '1947.0', '1935.0', '1935.0', '1887.0', '1882.0', '1957.0', '1887.0', \"'SEP'\", '1915.0', '357.0', '357.0', '357.0', '1941.0', '1947.0', '1947.0', '1935.0', '1935.0', '1887.0', '1887.0', '1413.0', '1957.0', '1887.0', \"'SEP'\", '1947.0', '1935.0', '1887.0', '1957.0', \"'SEP'\", '1947.0', '1935.0', '1935.0', '1887.0', '1957.0', '1887.0', \"'SEP'\", '1947.0', '1935.0', '1935.0', '1887.0', '1957.0', '1887.0', \"'SEP'\", '1935.0', '1947.0', '1935.0', '1887.0', '1957.0', \"'SEP'\", '1935.0', \"'SEP'\", '1935.0', \"'SEP'\", '1943.0', '1941.0', '1947.0', '1947.0', '1949.0', '1935.0', '1887.0', '2016.0', '1934.0', '1903.0', '1882.0', '1957.0', '1887.0', \"'SEP'\", '1941.0', '1947.0', '1935.0', '1887.0', '1957.0', \"'SEP'\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'diagnosis_code' is a column in your DataFrame\n",
    "original_data['diagnosis_code'] = original_data['diagnosis_code'].apply(lambda x: ''.join([char for char in str(x) if (char != ' ' and char != '[' and char != ']')]).split(','))\n",
    "print(original_data['diagnosis_code'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             deid_pat_ID  \\\n",
      "0           0      IRB202001139_PAT_1   \n",
      "1           1     IRB202001139_PAT_10   \n",
      "2           2  IRB202001139_PAT_10001   \n",
      "3           3  IRB202001139_PAT_10002   \n",
      "4           4  IRB202001139_PAT_10009   \n",
      "\n",
      "                                          age_vector  \\\n",
      "0  [31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 3...   \n",
      "1  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4  [66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 6...   \n",
      "\n",
      "                                      diagnosis_code  \n",
      "0  ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...  \n",
      "1  ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...  \n",
      "2  ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...  \n",
      "3  ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...  \n",
      "4  ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...  \n"
     ]
    }
   ],
   "source": [
    "print(original_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test datasets\n",
    "\n",
    "file_config = {\n",
    "    'vocab': '/home/leyang.sun/BERHT/BEHRT/saved_vocab', # token2idx idx2token\n",
    "    'train': '/home/leyang.sun/BERHT/BEHRT/train_data.parquet',\n",
    "    'test': '/home/leyang.sun/BERHT/BEHRT/test_data.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ cls, 1,2, sep, 3,4 , sep, 4,5, sep, 2,4, sep,2,sep]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def process_patient_data(row):\n",
    "    # Count the number of visits for the patient\n",
    "    total_visits = row['diagnosis_code'].count(\"'SEP'\") \n",
    "\n",
    "    # Check if total visits is greater than 3\n",
    "    if total_visits <= 3:\n",
    "        return None\n",
    "\n",
    "    # Choose a random index j for each patient (3 <= j < total_visits)\n",
    "    j = np.random.randint(3, total_visits) # j =4\n",
    "    # print(j)\n",
    "\n",
    "    # Create x_p: visits from 1 to j\n",
    "    x_p = row['age_vector'][:j]\n",
    "\n",
    "    # Find the (j-1)th and jth 'SEP' indices\n",
    "\n",
    "    # Assuming diagnosis_code is a list of strings and numbers\n",
    "    converted_diagnosis_code = []\n",
    "\n",
    "#     for code in row['diagnosis_code']:\n",
    "#         print(code)   \n",
    "        \n",
    "#     print(len(row['diagnosis_code']))\n",
    "    sep_indices = [i for i in range(len(row['diagnosis_code'])) if 'SEP' in str(row['diagnosis_code'][i])] # [3,6,9,12,14]\n",
    "#     print(sep_indices)\n",
    "#     print(j-2)\n",
    "    sep_indices_j_minus_1 = sep_indices[j - 2] # sep_indices[2] = 9\n",
    "    sep_indices_j = sep_indices[j - 1]\n",
    "\n",
    "    # Create label: diagnosis_code after the (j-1)th 'SEP' and before the jth 'SEP'\n",
    "    label = row['diagnosis_code'][sep_indices_j_minus_1 + 1:sep_indices_j]\n",
    "\n",
    "    # Delete elements after the jth 'SEP'\n",
    "    row['diagnosis_code'] = row['diagnosis_code'][:sep_indices_j]\n",
    "\n",
    "    return pd.Series({'deid_pat_ID': row['deid_pat_ID'], 'age_vector': x_p, 'diagnosis_code': row['diagnosis_code'], 'label': label})\n",
    "\n",
    "# Apply the function to each row of the original data\n",
    "processed_data = original_data.apply(process_patient_data, axis=1)\n",
    "\n",
    "# Drop rows where total visits is less than or equal to 3\n",
    "processed_data = processed_data.dropna()\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "processed_data_df = pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               deid_pat_ID                                         age_vector  \\\n",
      "0       IRB202001139_PAT_1                                   [31, 31, 31, 31]   \n",
      "1      IRB202001139_PAT_10  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2   IRB202001139_PAT_10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3   IRB202001139_PAT_10002  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4   IRB202001139_PAT_10009                           [66, 66, 66, 66, 67, 67]   \n",
      "6   IRB202001139_PAT_10014                                       [64, 65, 68]   \n",
      "7   IRB202001139_PAT_10018  [58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 5...   \n",
      "8   IRB202001139_PAT_10020                                       [83, 83, 83]   \n",
      "9   IRB202001139_PAT_10025  [48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...   \n",
      "10  IRB202001139_PAT_10034  [48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 5...   \n",
      "11  IRB202001139_PAT_10037                                       [46, 46, 46]   \n",
      "12  IRB202001139_PAT_10039  [21, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...   \n",
      "13   IRB202001139_PAT_1004                           [30, 32, 32, 32, 32, 32]   \n",
      "14  IRB202001139_PAT_10040   [77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78]   \n",
      "15  IRB202001139_PAT_10041  [59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 6...   \n",
      "16  IRB202001139_PAT_10046                               [62, 62, 62, 62, 62]   \n",
      "17  IRB202001139_PAT_10047                                       [45, 45, 45]   \n",
      "18  IRB202001139_PAT_10048  [81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 8...   \n",
      "19  IRB202001139_PAT_10054                   [69, 69, 69, 69, 69, 69, 69, 69]   \n",
      "21  IRB202001139_PAT_10056  [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 5...   \n",
      "\n",
      "                                       diagnosis_code  \\\n",
      "0   ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...   \n",
      "1   ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...   \n",
      "2   ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...   \n",
      "3   ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...   \n",
      "4   ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...   \n",
      "6   ['CLS', 1532.0, 1696.0, 1696.0, 'SEP', 760.0, ...   \n",
      "7   ['CLS', 570.0, 'SEP', 570.0, 'SEP', 760.0, 150...   \n",
      "8   ['CLS', 236.0, 760.0, 1564.0, 1696.0, 'SEP', 4...   \n",
      "9   ['CLS', 345.0, 499.0, 760.0, 775.0, 775.0, 814...   \n",
      "10  ['CLS', 345.0, 398.0, 761.0, 1696.0, 'SEP', 66...   \n",
      "11  ['CLS', 540.0, 760.0, 840.0, 844.0, 865.0, 865...   \n",
      "12  ['CLS', 1720.0, 1720.0, 'SEP', 71.0, 1038.0, 1...   \n",
      "13  ['CLS', 936.0, 1271.0, 1320.0, 1696.0, 71.0, 1...   \n",
      "14  ['CLS', 653.0, 760.0, 1145.0, 1158.0, 1204.0, ...   \n",
      "15  ['CLS', 1902.0, 1902.0, 1920.0, 1920.0, 345.0,...   \n",
      "16  ['CLS', 126.0, 126.0, 'SEP', 126.0, 741.0, 'SE...   \n",
      "17  ['CLS', 176.0, 250.0, 570.0, 829.0, 1753.0, 17...   \n",
      "18  ['CLS', 557.0, 'SEP', 557.0, 'SEP', 557.0, 'SE...   \n",
      "19  ['CLS', 328.0, 366.0, 366.0, 415.0, 492.0, 502...   \n",
      "21  ['CLS', 10.0, 760.0, 791.0, 1867.0, 1862.0, 35...   \n",
      "\n",
      "                                                label  \n",
      "0                                             [579.0]  \n",
      "1   [366.0, 366.0, 1947.0, 1948.0, 1956.0, 1903.0,...  \n",
      "2                             [677.0, 1530.0, 1656.0]  \n",
      "3                            [1919.0, 1975.0, 1978.0]  \n",
      "4   [1862.0, 1869.0, 1867.0, 1911.0, 1915.0, 1915....  \n",
      "6   [1928.0, 1928.0, 1930.0, 1929.0, 1928.0, 1883....  \n",
      "7                                    [1920.0, 1903.0]  \n",
      "8                                      [418.0, 914.0]  \n",
      "9                        [345.0, 760.0, 775.0, 830.0]  \n",
      "10                                   [1920.0, 1920.0]  \n",
      "11                      [693.0, 670.0, 865.0, 1700.0]  \n",
      "12                                   [1320.0, 1331.0]  \n",
      "13           [1519.0, 1532.0, 1706.0, 1707.0, 1707.0]  \n",
      "14                           [1204.0, 1214.0, 1653.0]  \n",
      "15                                           [1891.0]  \n",
      "16                              [126.0, 126.0, 760.0]  \n",
      "17  [176.0, 250.0, 418.0, 607.0, 612.0, 1117.0, 17...  \n",
      "18                                     [553.0, 557.0]  \n",
      "19                             [553.0, 760.0, 1696.0]  \n",
      "21                                   [1887.0, 1887.0]  \n"
     ]
    }
   ],
   "source": [
    "print(processed_data_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_elements = sorted(set([item for sublist in processed_data_df['label'] for item in sublist]))\n",
    "\n",
    "# Create a new DataFrame with one-hot encoding\n",
    "one_hot_df = pd.DataFrame()\n",
    "for element in unique_elements:\n",
    "    one_hot_df[element] = processed_data_df['label'].apply(lambda x: '1' if element in x else '0')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(one_hot_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              deid_pat_ID                                         age_vector  \\\n",
      "0      IRB202001139_PAT_1                                   [31, 31, 31, 31]   \n",
      "1     IRB202001139_PAT_10  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2  IRB202001139_PAT_10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  IRB202001139_PAT_10002  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4  IRB202001139_PAT_10009                           [66, 66, 66, 66, 67, 67]   \n",
      "\n",
      "                                      diagnosis_code  \\\n",
      "0  ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...   \n",
      "1  ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...   \n",
      "2  ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...   \n",
      "3  ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...   \n",
      "4  ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...   \n",
      "\n",
      "                                               label  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "for index, row in one_hot_df.iterrows():\n",
    "    label_list = row.tolist()\n",
    "    processed_data_df.at[index, 'label'] = label_list\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(processed_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the length of the longest list in the 'label' column\n",
    "# max_length = max(len(lst) for lst in processed_data_df['label'])\n",
    "\n",
    "# # Extend each list to the length of the longest list and fill with zeros\n",
    "# processed_data_df['label'] = processed_data_df['label'].apply(lambda x: x + ['0'] * (max_length - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   deid_pat_ID                                         age_vector  \\\n",
      "0            1                                   [31, 31, 31, 31]   \n",
      "1           10  [69, 69, 70, 70, 70, 70, 70, 70, 71, 71, 71, 7...   \n",
      "2        10001  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3        10002  [19, 19, 20, 20, 21, 21, 21, 22, 22, 22, 22, 2...   \n",
      "4        10009                           [66, 66, 66, 66, 67, 67]   \n",
      "6        10014                                       [64, 65, 68]   \n",
      "7        10018  [58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 5...   \n",
      "8        10020                                       [83, 83, 83]   \n",
      "9        10025  [48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 4...   \n",
      "10       10034  [48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 5...   \n",
      "11       10037                                       [46, 46, 46]   \n",
      "12       10039  [21, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 2...   \n",
      "13        1004                           [30, 32, 32, 32, 32, 32]   \n",
      "14       10040   [77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78]   \n",
      "15       10041  [59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 6...   \n",
      "16       10046                               [62, 62, 62, 62, 62]   \n",
      "17       10047                                       [45, 45, 45]   \n",
      "18       10048  [81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 8...   \n",
      "19       10054                   [69, 69, 69, 69, 69, 69, 69, 69]   \n",
      "21       10056  [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 5...   \n",
      "\n",
      "                                       diagnosis_code  \\\n",
      "0   ['CLS', 366.0, 366.0, 418.0, 705.0, 760.0, 838...   \n",
      "1   ['CLS', 1117.0, 1113.0, 65.0, 72.0, 366.0, 366...   \n",
      "2   ['CLS', 35.0, 920.0, 1696.0, 'SEP', 388.0, 'SE...   \n",
      "3   ['CLS', 492.0, 1531.0, 1534.0, 'SEP', 1707.0, ...   \n",
      "4   ['CLS', 250.0, 579.0, 590.0, 590.0, 760.0, 824...   \n",
      "6   ['CLS', 1532.0, 1696.0, 1696.0, 'SEP', 760.0, ...   \n",
      "7   ['CLS', 570.0, 'SEP', 570.0, 'SEP', 760.0, 150...   \n",
      "8   ['CLS', 236.0, 760.0, 1564.0, 1696.0, 'SEP', 4...   \n",
      "9   ['CLS', 345.0, 499.0, 760.0, 775.0, 775.0, 814...   \n",
      "10  ['CLS', 345.0, 398.0, 761.0, 1696.0, 'SEP', 66...   \n",
      "11  ['CLS', 540.0, 760.0, 840.0, 844.0, 865.0, 865...   \n",
      "12  ['CLS', 1720.0, 1720.0, 'SEP', 71.0, 1038.0, 1...   \n",
      "13  ['CLS', 936.0, 1271.0, 1320.0, 1696.0, 71.0, 1...   \n",
      "14  ['CLS', 653.0, 760.0, 1145.0, 1158.0, 1204.0, ...   \n",
      "15  ['CLS', 1902.0, 1902.0, 1920.0, 1920.0, 345.0,...   \n",
      "16  ['CLS', 126.0, 126.0, 'SEP', 126.0, 741.0, 'SE...   \n",
      "17  ['CLS', 176.0, 250.0, 570.0, 829.0, 1753.0, 17...   \n",
      "18  ['CLS', 557.0, 'SEP', 557.0, 'SEP', 557.0, 'SE...   \n",
      "19  ['CLS', 328.0, 366.0, 366.0, 415.0, 492.0, 502...   \n",
      "21  ['CLS', 10.0, 760.0, 791.0, 1867.0, 1862.0, 35...   \n",
      "\n",
      "                                                label  \n",
      "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "11  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "13  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "14  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "15  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "17  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "18  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "19  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "21  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "processed_data_df['deid_pat_ID'] = processed_data_df['deid_pat_ID'].str.replace('IRB202001139_PAT_', '', regex=False)\n",
    "# Display the revised DataFrame\n",
    "print(processed_data_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(processed_data_df, test_size=0.2, random_state=40)\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train_df = train_df.rename(columns={'age_vector': 'age', 'diagnosis_code': 'code', 'deid_pat_ID':'patid'})\n",
    "test_df = test_df.rename(columns={'age_vector': 'age', 'diagnosis_code': 'code', 'deid_pat_ID':'patid'})\n",
    "\n",
    "\n",
    "# Reset the index of train and test DataFrames\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save DataFrames as Parquet files without the index column\n",
    "train_df.to_parquet('/home/leyang.sun/BERHT/BEHRT/train_data.parquet', index=False)\n",
    "test_df.to_parquet('/home/leyang.sun/BERHT/BEHRT/test_data.parquet', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      patid                                                age  \\\n",
      "0     23430  [56, 57, 57, 57, 60, 60, 60, 60, 60, 60, 60, 6...   \n",
      "1      7202                                   [67, 67, 68, 68]   \n",
      "2     16812  [75, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 7...   \n",
      "3      5631  [67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 6...   \n",
      "4     46842                                   [49, 49, 49, 49]   \n",
      "...     ...                                                ...   \n",
      "6353   2705   [69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 72]   \n",
      "6354  44668                           [77, 77, 78, 78, 78, 78]   \n",
      "6355    438           [67, 67, 67, 67, 67, 67, 67, 67, 67, 67]   \n",
      "6356  44978                                   [18, 18, 18, 18]   \n",
      "6357  28866                                       [42, 42, 42]   \n",
      "\n",
      "                                                   code  \\\n",
      "0     ['CLS', 1523.0, 'SEP', 26.0, 1555.0, 1921.0, 1...   \n",
      "1     ['CLS', 760.0, 357.0, 357.0, 366.0, 366.0, 466...   \n",
      "2     ['CLS', 228.0, 612.0, 1700.0, 'SEP', 227.0, 34...   \n",
      "3     ['CLS', 760.0, 345.0, 830.0, 1696.0, 'SEP', 34...   \n",
      "4     ['CLS', 1930.0, 1928.0, 1883.0, 1947.0, 1946.0...   \n",
      "...                                                 ...   \n",
      "6353  ['CLS', 126.0, 250.0, 345.0, 1787.0, 'SEP', 12...   \n",
      "6354  ['CLS', 1883.0, 1883.0, 'SEP', 1883.0, 1883.0,...   \n",
      "6355  ['CLS', 1883.0, 541.0, 1966.0, 'SEP', 1883.0, ...   \n",
      "6356  ['CLS', 1915.0, 1916.0, 357.0, 366.0, 1883.0, ...   \n",
      "6357  ['CLS', 760.0, 775.0, 813.0, 857.0, 1181.0, 'S...   \n",
      "\n",
      "                                                  label  \n",
      "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "6353  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6355  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6356  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6357  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[6358 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      patid                                                age  \\\n",
      "0      3134  [76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 7...   \n",
      "1     18279  [75, 75, 75, 75, 75, 78, 78, 78, 78, 79, 79, 7...   \n",
      "2      7867  [38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 4...   \n",
      "3     11816  [70, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 7...   \n",
      "4      6341                           [79, 80, 83, 83, 84, 84]   \n",
      "...     ...                                                ...   \n",
      "1585  41714                                   [65, 65, 65, 65]   \n",
      "1586   4544                                       [82, 83, 83]   \n",
      "1587  20257  [38, 38, 39, 39, 41, 41, 42, 42, 42, 42, 42, 4...   \n",
      "1588  45164  [56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 5...   \n",
      "1589  32501  [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 3...   \n",
      "\n",
      "                                                   code  \\\n",
      "0     ['CLS', 236.0, 345.0, 556.0, 558.0, 813.0, 'SE...   \n",
      "1     ['CLS', 250.0, 577.0, 760.0, 777.0, 844.0, 150...   \n",
      "2     ['CLS', 421.0, 415.0, 273.0, 1181.0, 1198.0, 1...   \n",
      "3     ['CLS', 1014.0, 'SEP', 177.0, 'SEP', 177.0, 'S...   \n",
      "4     ['CLS', 250.0, 612.0, 603.0, 760.0, 775.0, 151...   \n",
      "...                                                 ...   \n",
      "1585  ['CLS', 1901.0, 1883.0, 667.0, 2016.0, 'SEP', ...   \n",
      "1586         ['CLS', 556.0, 'SEP', 556.0, 'SEP', 556.0]   \n",
      "1587  ['CLS', 282.0, 366.0, 760.0, 813.0, 840.0, 121...   \n",
      "1588  ['CLS', 35.0, 'SEP', 1901.0, 1880.0, 1956.0, 4...   \n",
      "1589  ['CLS', 1867.0, 1915.0, 345.0, 366.0, 1883.0, ...   \n",
      "\n",
      "                                                  label  \n",
      "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "1585  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1586  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1587  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1588  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1589  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[1590 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {\n",
    "    'lr': 1e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 256,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': '/home/leyang.sun/BERHT/BEHRT/fine_tuned_model',# output folder\n",
    "    'best_name': 'FineTuned_BERT_Large_Nextvisit',  # output model name\n",
    "    'max_len_seq': 100,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "pretrain_model_path = '/home/leyang.sun/BERHT/BEHRT/saved_model/BERT_Large_v1_2023-10-19'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new vocabulary for labels by removing specific tokens like 'PAD', 'SEP', 'CLS', and 'MASK' from the original vocabulary.\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    del token2idx['PAD']\n",
    "    del token2idx['SEP']\n",
    "    del token2idx['CLS']\n",
    "    del token2idx['MASK']\n",
    "    token = list(token2idx.keys())\n",
    "    labelVocab = {}\n",
    "    for i,x in enumerate(token):\n",
    "        labelVocab[x] = i\n",
    "    return labelVocab\n",
    "\n",
    "labelVocab = format_label_vocab(BertVocab['token2idx']) # return a dictionary called labelVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word':True,\n",
    "    'seg':True,\n",
    "    'age':True,\n",
    "    'position': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(file_config['train'])\n",
    "Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=train, max_len=global_params['max_len_seq'])\n",
    "trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(file_config['test'])\n",
    "Dset = NextVisit(token2idx=BertVocab['token2idx'], label2idx=labelVocab, age2idx=ageVocab, dataframe=test, max_len=global_params['max_len_seq'])\n",
    "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, num_labels=len(labelVocab.keys()), feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(path, model):\n",
    "#     # load pretrained model and update weights\n",
    "#     pretrained_dict = torch.load(path)\n",
    "#     model_dict = model.state_dict()\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict)\n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "#     return model\n",
    "\n",
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out unnecessary keys and skip the mismatched parameter\n",
    "    pretrained_dict = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in model_dict and k != 'bert.embeddings.posi_embeddings.weight'\n",
    "    }\n",
    "\n",
    "    # Update entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "\n",
    "    # Load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "mode = load_model(pretrain_model_path, model)  # Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = model.to(global_params['device'])\n",
    "optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, output, label\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "    roc = sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "    return tempprc, roc, output, label,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
       "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                             28, 29, ...])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, train_losses):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt +=1\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        \n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 500==0:\n",
    "            prec, a, b = precision(logits, targets)\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt,temp_loss/500, prec))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            \n",
    "    train_losses.append(tr_loss / nb_tr_steps)\n",
    "\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(testload):\n",
    "        model.eval()\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        y_label.append(targets)\n",
    "        y.append(logits)\n",
    "\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    aps, roc, output, label = precision_test(y, y_label)\n",
    "    return aps, roc, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Cnt: 1\t| Loss: 0.0013751969337463378\t| precision: 0.0014972900430714048\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "aps : 0.04508178709962442\n",
      "epoch: 1\t| Cnt: 1\t| Loss: 0.0011457746028900146\t| precision: 0.04745859302793386\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "aps : 1.0\n",
      "epoch: 2\t| Cnt: 1\t| Loss: 0.0008577525019645691\t| precision: 0.9609375\n",
      "aps : 1.0\n",
      "epoch: 3\t| Cnt: 1\t| Loss: 0.0006443355083465577\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 4\t| Cnt: 1\t| Loss: 0.0005106017589569092\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 5\t| Cnt: 1\t| Loss: 0.00042299506068229676\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 6\t| Cnt: 1\t| Loss: 0.00036126524209976194\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 7\t| Cnt: 1\t| Loss: 0.00031581714749336244\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 8\t| Cnt: 1\t| Loss: 0.00028068366646766664\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 9\t| Cnt: 1\t| Loss: 0.00025293710827827454\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 10\t| Cnt: 1\t| Loss: 0.00022981628775596617\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 11\t| Cnt: 1\t| Loss: 0.00021034862101078033\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 12\t| Cnt: 1\t| Loss: 0.0001941356062889099\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 13\t| Cnt: 1\t| Loss: 0.00018100497126579285\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 14\t| Cnt: 1\t| Loss: 0.00016781339049339294\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 15\t| Cnt: 1\t| Loss: 0.00015778684616088868\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 16\t| Cnt: 1\t| Loss: 0.00014811289310455322\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 17\t| Cnt: 1\t| Loss: 0.00014016905426979064\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 18\t| Cnt: 1\t| Loss: 0.00013303336501121522\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 19\t| Cnt: 1\t| Loss: 0.00012503054738044738\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 20\t| Cnt: 1\t| Loss: 0.00011935252696275711\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 21\t| Cnt: 1\t| Loss: 0.00011362534761428833\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 22\t| Cnt: 1\t| Loss: 0.00010863599181175232\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 23\t| Cnt: 1\t| Loss: 0.00010299882292747498\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 24\t| Cnt: 1\t| Loss: 9.923826158046722e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 25\t| Cnt: 1\t| Loss: 9.586893767118454e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 26\t| Cnt: 1\t| Loss: 9.150518476963044e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 27\t| Cnt: 1\t| Loss: 8.790605515241623e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 28\t| Cnt: 1\t| Loss: 8.482197672128677e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 29\t| Cnt: 1\t| Loss: 8.157462626695633e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 30\t| Cnt: 1\t| Loss: 7.776942104101181e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 31\t| Cnt: 1\t| Loss: 7.601473480463028e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 32\t| Cnt: 1\t| Loss: 7.277952134609222e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 33\t| Cnt: 1\t| Loss: 7.09126740694046e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 34\t| Cnt: 1\t| Loss: 6.858527660369873e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 35\t| Cnt: 1\t| Loss: 6.602045893669128e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 36\t| Cnt: 1\t| Loss: 6.400105357170104e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 37\t| Cnt: 1\t| Loss: 6.182050704956054e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 38\t| Cnt: 1\t| Loss: 6.000816076993942e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 39\t| Cnt: 1\t| Loss: 5.864204466342926e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 40\t| Cnt: 1\t| Loss: 5.67828007042408e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 41\t| Cnt: 1\t| Loss: 5.5552486330270766e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 42\t| Cnt: 1\t| Loss: 5.390565469861031e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 43\t| Cnt: 1\t| Loss: 5.242086574435234e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 44\t| Cnt: 1\t| Loss: 5.076617002487183e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 45\t| Cnt: 1\t| Loss: 4.9657560884952547e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 46\t| Cnt: 1\t| Loss: 4.8226919025182726e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 47\t| Cnt: 1\t| Loss: 4.6969152987003325e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 48\t| Cnt: 1\t| Loss: 4.593120515346527e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 49\t| Cnt: 1\t| Loss: 4.4557571411132814e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 50\t| Cnt: 1\t| Loss: 4.405157640576362e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 51\t| Cnt: 1\t| Loss: 4.2748060077428815e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 52\t| Cnt: 1\t| Loss: 4.157681763172149e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 53\t| Cnt: 1\t| Loss: 4.0562611073255536e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 54\t| Cnt: 1\t| Loss: 3.9490945637226106e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 55\t| Cnt: 1\t| Loss: 3.851606696844101e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 56\t| Cnt: 1\t| Loss: 3.801152855157852e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 57\t| Cnt: 1\t| Loss: 3.680714219808579e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 58\t| Cnt: 1\t| Loss: 3.601230680942535e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 59\t| Cnt: 1\t| Loss: 3.539589792490005e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 60\t| Cnt: 1\t| Loss: 3.436572104692459e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 61\t| Cnt: 1\t| Loss: 3.397887200117111e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 62\t| Cnt: 1\t| Loss: 3.288183361291885e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 63\t| Cnt: 1\t| Loss: 3.223457932472229e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 64\t| Cnt: 1\t| Loss: 3.1445659697055814e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 65\t| Cnt: 1\t| Loss: 3.069647215306759e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 66\t| Cnt: 1\t| Loss: 3.021165356040001e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 67\t| Cnt: 1\t| Loss: 2.9893716797232626e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 68\t| Cnt: 1\t| Loss: 2.9110146686434745e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 69\t| Cnt: 1\t| Loss: 2.8546888381242753e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 70\t| Cnt: 1\t| Loss: 2.799275517463684e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 71\t| Cnt: 1\t| Loss: 2.7490396052598953e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 72\t| Cnt: 1\t| Loss: 2.706670016050339e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 73\t| Cnt: 1\t| Loss: 2.622126042842865e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 74\t| Cnt: 1\t| Loss: 2.5956392288208008e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 75\t| Cnt: 1\t| Loss: 2.5506988167762755e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 76\t| Cnt: 1\t| Loss: 2.4893973022699357e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 77\t| Cnt: 1\t| Loss: 2.459540218114853e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 78\t| Cnt: 1\t| Loss: 2.4029228836297987e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 79\t| Cnt: 1\t| Loss: 2.3645408451557158e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 80\t| Cnt: 1\t| Loss: 2.3104224354028703e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 81\t| Cnt: 1\t| Loss: 2.263408713042736e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 82\t| Cnt: 1\t| Loss: 2.2227955982089042e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 83\t| Cnt: 1\t| Loss: 2.1872656419873237e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 84\t| Cnt: 1\t| Loss: 2.13431753218174e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 85\t| Cnt: 1\t| Loss: 2.0956650376319885e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 86\t| Cnt: 1\t| Loss: 2.0829115062952042e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 87\t| Cnt: 1\t| Loss: 2.0386062562465666e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 88\t| Cnt: 1\t| Loss: 1.9883304834365845e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 89\t| Cnt: 1\t| Loss: 1.958707347512245e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 90\t| Cnt: 1\t| Loss: 1.9228361546993254e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 91\t| Cnt: 1\t| Loss: 1.9177444279193878e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 92\t| Cnt: 1\t| Loss: 1.8576754257082938e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 93\t| Cnt: 1\t| Loss: 1.8287304788827897e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 94\t| Cnt: 1\t| Loss: 1.7989762127399445e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 95\t| Cnt: 1\t| Loss: 1.7862744629383086e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 96\t| Cnt: 1\t| Loss: 1.7336167395114898e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 97\t| Cnt: 1\t| Loss: 1.7355373129248618e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 98\t| Cnt: 1\t| Loss: 1.6989551484584808e-05\t| precision: 1.0\n",
      "aps : 1.0\n",
      "epoch: 99\t| Cnt: 1\t| Loss: 1.6742216423153877e-05\t| precision: 1.0\n",
      "aps : 1.0\n"
     ]
    }
   ],
   "source": [
    "best_pre = 0.0\n",
    "train_losses = []\n",
    "for e in range(100):\n",
    "    train(e, train_losses)\n",
    "    aps, roc, test_loss = evaluation()\n",
    "    if aps >best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = aps\n",
    "    print('aps : {}'.format(aps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # Import the matplotlib library\n",
    "\n",
    "# Plotting the training losses after each epoch\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-leyang.sun",
   "language": "python",
   "name": "leyang.sun-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
